{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945ac8da-22f6-4564-a478-8ebf1d4c8bb6",
   "metadata": {},
   "source": [
    "GRID SEARCH?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f77effec-1a3c-4dbe-9bfb-691df61e1ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRanker\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.metrics import ndcg_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_test_split_by_group(data, id_column, test_size=0.2, random_state=None):\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    groups = data[id_column]\n",
    "    train_idx, test_idx = next(gss.split(data, groups=groups))\n",
    "    train_set = data.iloc[train_idx]\n",
    "    test_set = data.iloc[test_idx]\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "def preprocess_data(train_set, test_set, test_label=False):\n",
    "    train_set = train_set.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    test_set = test_set.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    X_train = train_set.loc[:, ~train_set.columns.isin(['srch_id','target_label', 'position', 'gross_booking_usd'])]\n",
    "    y_train = train_set['target_label']\n",
    "    qid_train = train_set['srch_id']\n",
    "    X_test = test_set.loc[:, ~test_set.columns.isin(['srch_id','target_label', 'position', 'gross_booking_usd'])]\n",
    "    if test_label:\n",
    "        y_test = test_set['target_label']\n",
    "    else:\n",
    "        y_test = None\n",
    "    qid_test = test_set['srch_id']\n",
    "\n",
    "    return X_train, y_train, qid_train, X_test, y_test, qid_test\n",
    "\n",
    "\n",
    "def train_lgbm_ranker(X_train, y_train, train_groups, params=None):\n",
    "    if params is None:\n",
    "        params = {\n",
    "            \"objective\": \"lambdarank\",\n",
    "            \"metric\": \"ndcg\",\n",
    "            \"n_estimators\": 2000,\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"verbose\": -1\n",
    "        }\n",
    "\n",
    "    gbm = LGBMRanker(**params)\n",
    "    gbm.fit(X_train, y_train, group=train_groups)\n",
    "\n",
    "    return gbm\n",
    "\n",
    "\n",
    "def predict_and_generate_submission(model, X_test, q_id_test, output_file):\n",
    "    predictions = []\n",
    "    for group in tqdm(np.unique(q_id_test), desc='Processing groups'):\n",
    "        preds = model.predict(X_test[q_id_test == group])\n",
    "        predictions.extend(preds)\n",
    "\n",
    "    X_test['preds'] = predictions\n",
    "    X_test['srch_id'] = q_id_test\n",
    "\n",
    "    result = X_test.sort_values(by=['srch_id', 'preds'], ascending=[True, False])\n",
    "    result[['srch_id', 'prop_id']].reset_index(drop=True).to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_ndcg(model, X_test, y_test, q_id_test, k=5, use_tqdm = True):\n",
    "\n",
    "    \n",
    "    ndcg_scores = []\n",
    "    qids = np.unique(q_id_test)\n",
    "\n",
    "    iterator = tqdm(qids, desc='Calculating NDCG scores') if use_tqdm else qids\n",
    "    \n",
    "    for qid in iterator:\n",
    "        y = y_test[q_id_test == qid].values.flatten()\n",
    "        p = model.predict(X_test[q_id_test == qid])\n",
    "        ndcg_scores.append(ndcg_score([y], [p], k=k))\n",
    "    return np.mean(ndcg_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c7f1f9c-43fb-43ec-986e-5c9da4a58fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_path = './dataset/train_new_feature.csv'\n",
    "test_set_path = './dataset/test_new_feature.csv'\n",
    "\n",
    "train_set, test_set = train_test_split_by_group(pd.read_csv(train_set_path), id_column='srch_id', test_size=0.1, random_state= 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "426b286d-41cf-4894-9cb8-282e2e99853c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "srch_id\n",
       "1         28\n",
       "4         32\n",
       "6          5\n",
       "8         21\n",
       "11        33\n",
       "          ..\n",
       "332776    10\n",
       "332777    32\n",
       "332781    15\n",
       "332782    24\n",
       "332785     6\n",
       "Name: count, Length: 179815, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, qid_train, X_test, y_test, qid_test = preprocess_data(train_set, test_set, test_label=True)\n",
    "qid_train.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b743895c-b5cd-4935-a931-54a9a2dea6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.156598 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2976\n",
      "[LightGBM] [Info] Number of data points in the train set: 4461236, number of used features: 21\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": \"ndcg\",\n",
    "    \"n_estimators\": 100,\n",
    "    \"learning_rate\": 0.1\n",
    "}\n",
    "model = train_lgbm_ranker(X_train, y_train, qid_train.value_counts().sort_index(), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a17f898-3276-4f58-a0e9-79d95584cdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating NDCG scores: 100%|â–ˆ| 19980/19980 [00:31<00:00, 638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37994632886014024"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_ndcg(model, X_test, y_test, qid_test, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b76d9-2bf2-487b-aae7-b281b51c7aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing parameter combinations:   0%| | 1/324 [02:10<11:43:44,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters yet: {'objective': 'lambdarank', 'learning_rate': 0.01, 'num_leaves': 31, 'min_data_in_leaf': 20, 'lambda_l2': 0, 'max_bin': 256, 'n_estimators': 500, 'metric': ['ndcg'], 'verbose': -1}\n",
      "The best ndcg_score yet: 0.3734379328932991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing parameter combinations:   1%| | 2/324 [05:45<16:06:47,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters yet: {'objective': 'lambdarank', 'learning_rate': 0.01, 'num_leaves': 31, 'min_data_in_leaf': 20, 'lambda_l2': 0, 'max_bin': 256, 'n_estimators': 1000, 'metric': ['ndcg'], 'verbose': -1}\n",
      "The best ndcg_score yet: 0.3799843565015373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing parameter combinations:   1%| | 4/324 [11:37<16:16:43,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters yet: {'objective': 'lambdarank', 'learning_rate': 0.01, 'num_leaves': 31, 'min_data_in_leaf': 20, 'lambda_l2': 0, 'max_bin': 512, 'n_estimators': 1000, 'metric': ['ndcg'], 'verbose': -1}\n",
      "The best ndcg_score yet: 0.3810214954272664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing parameter combinations:   4%| | 12/324 [35:08<15:48:34"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters yet: {'objective': 'lambdarank', 'learning_rate': 0.01, 'num_leaves': 31, 'min_data_in_leaf': 20, 'lambda_l2': 1.0, 'max_bin': 512, 'n_estimators': 1000, 'metric': ['ndcg'], 'verbose': -1}\n",
      "The best ndcg_score yet: 0.38104075651368263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing parameter combinations:   6%| | 20/324 [58:48<15:53:50"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "\n",
    "\n",
    "def grid_search_lgbm_ranker(X_train, y_train, train_groups, X_test, y_test, q_id_test, param_grid = None):\n",
    "    best_ndcg = -np.inf\n",
    "    best_model = None\n",
    "    best_params = {}\n",
    "\n",
    "    if not param_grid:\n",
    "        param_grid = {\n",
    "            'objective': ['lambdarank'],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'num_leaves': [31, 64, 256],\n",
    "            'min_data_in_leaf': [20, 100, 200],\n",
    "            'lambda_l2': [0, 0.5, 1.0],\n",
    "            'max_bin': [256, 512],\n",
    "            'n_estimators': [500, 1000],\n",
    "            'metric': [['ndcg']],\n",
    "            \"verbose\": [-1]\n",
    "        }\n",
    "        \n",
    "\n",
    "    from itertools import product\n",
    "    keys, values = zip(*param_grid.items())\n",
    "\n",
    "    # All combination of parameters\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "    for params in tqdm(param_combinations, desc='Testing parameter combinations'):\n",
    "        model = train_lgbm_ranker(X_train, y_train, train_groups, params)\n",
    "        \n",
    "        ndcg_score = calculate_ndcg(model, X_test, y_test, q_id_test, k=5, use_tqdm = False)\n",
    "        \n",
    "        if ndcg_score > best_ndcg:\n",
    "            best_ndcg = ndcg_score\n",
    "            best_model = model\n",
    "            best_params = params\n",
    "\n",
    "            print(f\"The best parameters yet: {best_params}\")\n",
    "            print(f\"The best ndcg_score yet: {best_ndcg}\")\n",
    "\n",
    "    return best_model, best_params, best_ndcg\n",
    "\n",
    "\n",
    "train_groups = qid_train.value_counts().sort_index()\n",
    "\n",
    "best_model, best_params, best_ndcg_score = grid_search_lgbm_ranker(\n",
    "    X_train, y_train, train_groups, X_test, y_test, qid_test\n",
    ")\n",
    "\n",
    "\n",
    "# predict_and_generate_submission(best_model, X_test, qid_test, \"output_predictions.csv\")\n",
    "\n",
    "print(\"Best Params:\", best_params)\n",
    "print(\"Best NDCG Score:\", best_ndcg_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e3f528-8520-4aa1-ab84-fde216a39732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_lightgbm)",
   "language": "python",
   "name": "venv_lightgbm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
