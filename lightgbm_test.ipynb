{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-14T21:21:00.104378Z",
     "start_time": "2024-05-14T21:20:54.834156Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import xgboost as xgb\n",
    "\n",
    "def train_test_split_by_group(data, id_column, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split a dataset by groups defined by a specific column.\n",
    "    This function is used to split the queries into train and test set\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame, the dataset to be split.\n",
    "    - id_column: str, the name of the column containing the group IDs.\n",
    "    - test_size: float, optional (default=0.2), the proportion of the dataset to include in the test split.\n",
    "    - random_state: int or RandomState instance, optional (default=None), control the randomness of the shuffling.\n",
    "\n",
    "    Returns:\n",
    "    - train_set: pandas DataFrame, the training set.\n",
    "    - test_set: pandas DataFrame, the test set.\n",
    "    \"\"\"\n",
    "    # Create GroupShuffleSplit object\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Group by the specified column and apply GroupShuffleSplit\n",
    "    groups = data[id_column]\n",
    "    train_idx, test_idx = next(gss.split(data, groups=groups))\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    train_set = data.iloc[train_idx]\n",
    "    test_set = data.iloc[test_idx]\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "df = pd.read_csv('./dataset/train_clean_v1.csv')\n",
    "train_set, test_set = train_test_split_by_group(df, 'srch_id', test_size=0.4, random_state=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2975504\n"
     ]
    }
   ],
   "source": [
    "X_train = train_set.loc[:, ~train_set.columns.isin(['srch_id','target_label'])]\n",
    "y_train = train_set.loc[:, train_set.columns.isin(['target_label'])]\n",
    "\n",
    "q_id_train = train_set['srch_id']\n",
    "q_id_test = test_set['srch_id']\n",
    "\n",
    "groups = train_set.groupby('srch_id').size()\n",
    "\n",
    "#We need to keep the id for later predictions\n",
    "X_test = test_set.loc[:, ~test_set.columns.isin(['srch_id','target_label'])]\n",
    "y_test = test_set.loc[:, test_set.columns.isin(['target_label'])]\n",
    "print(groups.sum())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T21:21:00.287795Z",
     "start_time": "2024-05-14T21:21:00.117712Z"
    }
   },
   "id": "f75bd0879597b15a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2125\n",
      "[LightGBM] [Info] Number of data points in the train set: 2975504, number of used features: 19\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRanker\n",
    "\n",
    "gbm = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.12,\n",
    ")\n",
    "gbm.fit(X_train, y_train, group=groups)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-14T21:21:00.289082Z"
    }
   },
   "id": "83ca139c519ef682"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "ndcg_ = list()\n",
    "qids = np.unique(q_id_test)\n",
    "for i, qid in enumerate(qids):\n",
    "    y = y_test[q_id_test == qid].values.flatten()\n",
    "\n",
    "    # if (y == 0).any():\n",
    "    #     continue\n",
    "    \n",
    "    p = gbm.predict(X_test[q_id_test == qid])\n",
    "    ndcg_.append(ndcg_score([y], [p], k=5))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ae02b7b555a4bd6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.mean(ndcg_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6e6a0752cb72d018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ecebc89467f012e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
